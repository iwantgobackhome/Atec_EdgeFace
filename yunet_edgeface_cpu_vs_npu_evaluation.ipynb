{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YuNet + EdgeFace CPU vs NPU 종합 평가\n",
    "\n",
    "이 노트북은 YuNet 얼굴 검출기와 EdgeFace 임베딩 모델의 CPU/NPU 버전을 LFW 데이터셋으로 종합 평가합니다.\n",
    "\n",
    "## 평가 목표\n",
    "1. **YuNet 검출 성능 비교**: CPU vs NPU 얼굴 검출 정확도, landmark 정확도\n",
    "2. **EdgeFace 임베딩 비교**: PyTorch vs NPU 임베딩 유사도\n",
    "3. **End-to-End 파이프라인**: YuNet+EdgeFace 전체 파이프라인 성능 비교\n",
    "\n",
    "## 평가 메트릭\n",
    "### YuNet 검출기\n",
    "- Detection Rate: 얼굴 검출 성공률\n",
    "- Landmark Accuracy: Landmark 위치 정확도 (CPU vs NPU 차이)\n",
    "- Detection Threshold 영향 분석\n",
    "\n",
    "### EdgeFace 임베딩\n",
    "- Embedding Similarity: 같은 얼굴에 대한 CPU/NPU 임베딩 유사도\n",
    "- Embedding Distance: CPU/NPU 임베딩 간 L2 distance\n",
    "\n",
    "### End-to-End 파이프라인\n",
    "- ROC AUC: Face verification 정확도\n",
    "- Best Accuracy: 최적 threshold에서의 정확도\n",
    "- EER (Equal Error Rate)\n",
    "- Cross-compatibility: CPU 캡처 → NPU 인식, NPU 캡처 → CPU 인식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import cv2\n",
    "\n",
    "# PyTorch 관련\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# EdgeFace 백본 모델 임포트\n",
    "sys.path.insert(0, 'face_alignment')\n",
    "from backbones import get_model\n",
    "\n",
    "# YuNet 검출기 임포트\n",
    "from face_alignment.yunet import YuNetDetector\n",
    "try:\n",
    "    from face_alignment.yunet_npu import YuNetNPUDetector\n",
    "    YUNET_NPU_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"⚠ YuNet NPU not available\")\n",
    "    YUNET_NPU_AVAILABLE = False\n",
    "\n",
    "# EdgeFace NPU 임포트\n",
    "try:\n",
    "    from edgeface_npu_recognizer import EdgeFaceNPURecognizer\n",
    "    EDGEFACE_NPU_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"⚠ EdgeFace NPU not available\")\n",
    "    EDGEFACE_NPU_AVAILABLE = False\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"YuNet NPU available: {YUNET_NPU_AVAILABLE}\")\n",
    "print(f\"EdgeFace NPU available: {EDGEFACE_NPU_AVAILABLE}\")\n",
    "\n",
    "# 디바이스 설정\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LFW 데이터셋 경로 설정\n",
    "lfw_dir = \"/mnt/c/Users/Admin/Downloads/lfw-deepfunneled/lfw-deepfunneled\"\n",
    "pairs_file = \"/mnt/c/Users/Admin/Downloads/lfw-deepfunneled/pairs.csv\"\n",
    "\n",
    "# 모델 경로 설정\n",
    "YUNET_CPU_MODEL = \"models/face_detection_yunet_2023mar.onnx\"\n",
    "YUNET_NPU_MODEL = \"models/yunet_npu/face_detection_yunet_2023mar.dxnn\"\n",
    "EDGEFACE_PYTORCH_MODEL = \"checkpoints/edgeface_xs_gamma_06.pt\"\n",
    "EDGEFACE_NPU_MODEL = \"checkpoints/edgeface_npu/edgeface_xs_gamma_06.dxnn\"\n",
    "\n",
    "# 경로 확인\n",
    "if not os.path.exists(lfw_dir):\n",
    "    print(f\"⚠ Warning: LFW directory not found at {lfw_dir}\")\n",
    "else:\n",
    "    print(f\"✓ LFW directory found: {lfw_dir}\")\n",
    "\n",
    "if not os.path.exists(pairs_file):\n",
    "    print(f\"⚠ Warning: Pairs file not found at {pairs_file}\")\n",
    "else:\n",
    "    print(f\"✓ Pairs file found: {pairs_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lfw_pairs(pairs_file: str, lfw_dir: str) -> List[Tuple]:\n",
    "    \"\"\"\n",
    "    LFW pairs 파일을 로드합니다.\n",
    "    \n",
    "    Returns:\n",
    "        List of (is_same, img1_path, img2_path) tuples\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    if pairs_file.endswith('.csv'):\n",
    "        with open(pairs_file, 'r') as f:\n",
    "            lines = f.readlines()[1:]  # 헤더 스킵\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            if line.endswith(','):\n",
    "                # 같은 사람 쌍\n",
    "                parts = line.rstrip(',').split(',')\n",
    "                if len(parts) == 3:\n",
    "                    try:\n",
    "                        person = parts[0]\n",
    "                        img1_num = int(parts[1])\n",
    "                        img2_num = int(parts[2])\n",
    "                        \n",
    "                        img1_path = os.path.join(lfw_dir, person, f\"{person}_{img1_num:04d}.jpg\")\n",
    "                        img2_path = os.path.join(lfw_dir, person, f\"{person}_{img2_num:04d}.jpg\")\n",
    "                        pairs.append((True, img1_path, img2_path))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "            else:\n",
    "                # 다른 사람 쌍\n",
    "                parts = line.split(',')\n",
    "                if len(parts) == 4:\n",
    "                    try:\n",
    "                        person1 = parts[0]\n",
    "                        img1_num = int(parts[1])\n",
    "                        person2 = parts[2]\n",
    "                        img2_num = int(parts[3])\n",
    "                        \n",
    "                        img1_path = os.path.join(lfw_dir, person1, f\"{person1}_{img1_num:04d}.jpg\")\n",
    "                        img2_path = os.path.join(lfw_dir, person2, f\"{person2}_{img2_num:04d}.jpg\")\n",
    "                        pairs.append((False, img1_path, img2_path))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Pairs 로드\n",
    "if os.path.exists(pairs_file):\n",
    "    pairs = load_lfw_pairs(pairs_file, lfw_dir)\n",
    "    print(f\"Loaded {len(pairs)} pairs from LFW\")\n",
    "    \n",
    "    positive_pairs = sum(1 for p in pairs if p[0])\n",
    "    negative_pairs = len(pairs) - positive_pairs\n",
    "    print(f\"  Positive pairs (same person): {positive_pairs}\")\n",
    "    print(f\"  Negative pairs (different person): {negative_pairs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YuNet CPU 검출기\n",
    "print(\"\\n=== Initializing YuNet CPU ===\")\n",
    "yunet_cpu = YuNetDetector(YUNET_CPU_MODEL, device='cpu', crop_size=(112, 112))\n",
    "\n",
    "# YuNet NPU 검출기\n",
    "yunet_npu = None\n",
    "if YUNET_NPU_AVAILABLE and os.path.exists(YUNET_NPU_MODEL):\n",
    "    print(\"\\n=== Initializing YuNet NPU ===\")\n",
    "    yunet_npu = YuNetNPUDetector(YUNET_NPU_MODEL, device='npu', crop_size=(112, 112))\n",
    "else:\n",
    "    print(\"⚠ YuNet NPU model not available\")\n",
    "\n",
    "# EdgeFace PyTorch 모델\n",
    "print(\"\\n=== Initializing EdgeFace PyTorch ===\")\n",
    "model_name = 'edgeface_xs_gamma_06'\n",
    "edgeface_pytorch = get_model(model_name, fp16=False)\n",
    "edgeface_pytorch.load_state_dict(torch.load(EDGEFACE_PYTORCH_MODEL, map_location=device))\n",
    "edgeface_pytorch.to(device)\n",
    "edgeface_pytorch.eval()\n",
    "print(f\"✓ EdgeFace PyTorch loaded on {device}\")\n",
    "\n",
    "# EdgeFace NPU 모델\n",
    "edgeface_npu = None\n",
    "if EDGEFACE_NPU_AVAILABLE and os.path.exists(EDGEFACE_NPU_MODEL):\n",
    "    print(\"\\n=== Initializing EdgeFace NPU ===\")\n",
    "    edgeface_npu = EdgeFaceNPURecognizer(EDGEFACE_NPU_MODEL, model_name, device='npu')\n",
    "else:\n",
    "    print(\"⚠ EdgeFace NPU model not available\")\n",
    "\n",
    "print(\"\\n✓ All available models initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 평가 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embedding_pytorch(face_img: np.ndarray, model: torch.nn.Module, device: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    PyTorch 모델로 임베딩 추출\n",
    "    \n",
    "    Args:\n",
    "        face_img: BGR 이미지 (112x112x3)\n",
    "        model: PyTorch 모델\n",
    "        device: 디바이스\n",
    "    \n",
    "    Returns:\n",
    "        512-d embedding vector\n",
    "    \"\"\"\n",
    "    # Resize if needed\n",
    "    if face_img.shape[:2] != (112, 112):\n",
    "        face_img = cv2.resize(face_img, (112, 112))\n",
    "    \n",
    "    # BGR to RGB\n",
    "    img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Transpose to CHW\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    \n",
    "    # Convert to tensor\n",
    "    img_tensor = torch.from_numpy(img).unsqueeze(0).float().to(device)\n",
    "    \n",
    "    # Normalize\n",
    "    img_tensor.div_(255).sub_(0.5).div_(0.5)\n",
    "    \n",
    "    # Extract embedding\n",
    "    with torch.no_grad():\n",
    "        embedding = model(img_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    # L2 normalize\n",
    "    embedding = embedding / np.linalg.norm(embedding)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "def extract_embedding_npu(face_img: np.ndarray, recognizer) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    NPU 모델로 임베딩 추출\n",
    "    \n",
    "    Args:\n",
    "        face_img: BGR 이미지 (112x112x3)\n",
    "        recognizer: EdgeFaceNPURecognizer\n",
    "    \n",
    "    Returns:\n",
    "        512-d embedding vector\n",
    "    \"\"\"\n",
    "    return recognizer.extract_embedding(face_img)\n",
    "\n",
    "print(\"Embedding extraction functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. YuNet 검출 성능 평가 (CPU vs NPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_yunet_detection(pairs: List[Tuple], detector, detector_name: str, max_pairs: int = 500) -> Dict:\n",
    "    \"\"\"\n",
    "    YuNet 검출 성능 평가\n",
    "    \n",
    "    Returns:\n",
    "        Dict with detection statistics\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'detector': detector_name,\n",
    "        'total_images': 0,\n",
    "        'detected': 0,\n",
    "        'failed': 0,\n",
    "        'detection_times': [],\n",
    "        'face_counts': [],  # Number of faces detected per image\n",
    "        'confidences': [],\n",
    "        'aligned_faces': []  # Store aligned faces for later use\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n=== Evaluating {detector_name} Detection ===\")\n",
    "    \n",
    "    # Sample pairs for evaluation\n",
    "    sampled_pairs = pairs[:max_pairs] if max_pairs else pairs\n",
    "    \n",
    "    for is_same, img1_path, img2_path in tqdm(sampled_pairs, desc=f\"{detector_name} Detection\"):\n",
    "        for img_path in [img1_path, img2_path]:\n",
    "            if not os.path.exists(img_path):\n",
    "                results['failed'] += 1\n",
    "                continue\n",
    "            \n",
    "            results['total_images'] += 1\n",
    "            \n",
    "            try:\n",
    "                # Load image\n",
    "                pil_img = Image.open(img_path).convert('RGB')\n",
    "                \n",
    "                # Detect face\n",
    "                start_time = time.time()\n",
    "                aligned_face = detector.align(pil_img)\n",
    "                detection_time = time.time() - start_time\n",
    "                \n",
    "                results['detection_times'].append(detection_time)\n",
    "                \n",
    "                if aligned_face is not None:\n",
    "                    results['detected'] += 1\n",
    "                    results['aligned_faces'].append(aligned_face)\n",
    "                    \n",
    "                    # Get detection info (if available)\n",
    "                    faces = detector.detect_faces(pil_img)\n",
    "                    if faces is not None and len(faces) > 0:\n",
    "                        results['face_counts'].append(len(faces))\n",
    "                        # Get confidence of best face\n",
    "                        confidences = [f[-1] for f in faces]\n",
    "                        results['confidences'].append(max(confidences))\n",
    "                else:\n",
    "                    results['failed'] += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                results['failed'] += 1\n",
    "    \n",
    "    # Calculate statistics\n",
    "    results['detection_rate'] = results['detected'] / results['total_images'] if results['total_images'] > 0 else 0\n",
    "    results['avg_detection_time'] = np.mean(results['detection_times']) if results['detection_times'] else 0\n",
    "    results['avg_faces_per_image'] = np.mean(results['face_counts']) if results['face_counts'] else 0\n",
    "    results['avg_confidence'] = np.mean(results['confidences']) if results['confidences'] else 0\n",
    "    \n",
    "    print(f\"\\n{detector_name} Results:\")\n",
    "    print(f\"  Total images: {results['total_images']}\")\n",
    "    print(f\"  Detected: {results['detected']} ({results['detection_rate']:.2%})\")\n",
    "    print(f\"  Failed: {results['failed']}\")\n",
    "    print(f\"  Avg detection time: {results['avg_detection_time']:.4f}s\")\n",
    "    print(f\"  Avg faces per image: {results['avg_faces_per_image']:.2f}\")\n",
    "    print(f\"  Avg confidence: {results['avg_confidence']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate YuNet CPU\n",
    "yunet_cpu_results = evaluate_yunet_detection(pairs, yunet_cpu, \"YuNet CPU\", max_pairs=500)\n",
    "\n",
    "# Evaluate YuNet NPU\n",
    "yunet_npu_results = None\n",
    "if yunet_npu is not None:\n",
    "    yunet_npu_results = evaluate_yunet_detection(pairs, yunet_npu, \"YuNet NPU\", max_pairs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YuNet 검출 성능 비교 시각화\n",
    "if yunet_npu_results is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    metrics = [\n",
    "        ('Detection Rate', 'detection_rate', axes[0, 0]),\n",
    "        ('Avg Detection Time (s)', 'avg_detection_time', axes[0, 1]),\n",
    "        ('Avg Faces Per Image', 'avg_faces_per_image', axes[1, 0]),\n",
    "        ('Avg Confidence', 'avg_confidence', axes[1, 1])\n",
    "    ]\n",
    "    \n",
    "    for metric_name, metric_key, ax in metrics:\n",
    "        cpu_val = yunet_cpu_results[metric_key]\n",
    "        npu_val = yunet_npu_results[metric_key]\n",
    "        \n",
    "        bars = ax.bar(['CPU', 'NPU'], [cpu_val, npu_val], \n",
    "                      color=['blue', 'red'], alpha=0.7)\n",
    "        \n",
    "        ax.set_ylabel(metric_name, fontsize=11)\n",
    "        ax.set_title(f'YuNet {metric_name}', fontsize=12)\n",
    "        \n",
    "        # 값 레이블\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.4f}',\n",
    "                   ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # 차이 표시\n",
    "        diff = npu_val - cpu_val\n",
    "        diff_pct = (diff / cpu_val * 100) if cpu_val != 0 else 0\n",
    "        ax.text(0.5, 0.95, f'Diff: {diff:.4f} ({diff_pct:+.2f}%)',\n",
    "               transform=ax.transAxes, ha='center', va='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "               fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('yunet_cpu_vs_npu_detection.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"YuNet detection comparison saved to: yunet_cpu_vs_npu_detection.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Landmark 정확도 비교 (CPU vs NPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_landmarks(pairs: List[Tuple], detector_cpu, detector_npu, max_pairs: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    CPU와 NPU의 landmark 차이 분석\n",
    "    \n",
    "    Returns:\n",
    "        Dict with landmark comparison statistics\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'landmark_distances': [],  # L2 distance between CPU and NPU landmarks\n",
    "        'landmark_distances_per_point': [[] for _ in range(5)],  # Per landmark point\n",
    "        'both_detected': 0,\n",
    "        'only_cpu_detected': 0,\n",
    "        'only_npu_detected': 0,\n",
    "        'neither_detected': 0\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n=== Comparing Landmarks (CPU vs NPU) ===\")\n",
    "    \n",
    "    sampled_pairs = pairs[:max_pairs] if max_pairs else pairs\n",
    "    \n",
    "    for is_same, img1_path, img2_path in tqdm(sampled_pairs, desc=\"Landmark Comparison\"):\n",
    "        for img_path in [img1_path, img2_path]:\n",
    "            if not os.path.exists(img_path):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                pil_img = Image.open(img_path).convert('RGB')\n",
    "                \n",
    "                # CPU detection with landmarks\n",
    "                aligned_cpu, landmarks_cpu = detector_cpu.align(pil_img, return_landmarks=True)\n",
    "                \n",
    "                # NPU detection with landmarks\n",
    "                aligned_npu, landmarks_npu = detector_npu.align(pil_img, return_landmarks=True)\n",
    "                \n",
    "                # Check detection status\n",
    "                if aligned_cpu is not None and aligned_npu is not None:\n",
    "                    results['both_detected'] += 1\n",
    "                    \n",
    "                    # Compare landmarks\n",
    "                    if landmarks_cpu is not None and landmarks_npu is not None:\n",
    "                        landmarks_cpu = np.array(landmarks_cpu)\n",
    "                        landmarks_npu = np.array(landmarks_npu)\n",
    "                        \n",
    "                        # Overall L2 distance\n",
    "                        dist = np.linalg.norm(landmarks_cpu - landmarks_npu)\n",
    "                        results['landmark_distances'].append(dist)\n",
    "                        \n",
    "                        # Per-point distance\n",
    "                        for i in range(min(5, len(landmarks_cpu))):\n",
    "                            point_dist = np.linalg.norm(landmarks_cpu[i] - landmarks_npu[i])\n",
    "                            results['landmark_distances_per_point'][i].append(point_dist)\n",
    "                \n",
    "                elif aligned_cpu is not None:\n",
    "                    results['only_cpu_detected'] += 1\n",
    "                elif aligned_npu is not None:\n",
    "                    results['only_npu_detected'] += 1\n",
    "                else:\n",
    "                    results['neither_detected'] += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    # Calculate statistics\n",
    "    if results['landmark_distances']:\n",
    "        results['avg_landmark_distance'] = np.mean(results['landmark_distances'])\n",
    "        results['std_landmark_distance'] = np.std(results['landmark_distances'])\n",
    "        results['max_landmark_distance'] = np.max(results['landmark_distances'])\n",
    "        \n",
    "        # Per-point statistics\n",
    "        landmark_names = ['Left Eye', 'Right Eye', 'Nose', 'Left Mouth', 'Right Mouth']\n",
    "        results['per_point_stats'] = []\n",
    "        for i, name in enumerate(landmark_names):\n",
    "            if results['landmark_distances_per_point'][i]:\n",
    "                results['per_point_stats'].append({\n",
    "                    'name': name,\n",
    "                    'avg': np.mean(results['landmark_distances_per_point'][i]),\n",
    "                    'std': np.std(results['landmark_distances_per_point'][i]),\n",
    "                    'max': np.max(results['landmark_distances_per_point'][i])\n",
    "                })\n",
    "    \n",
    "    print(f\"\\nLandmark Comparison Results:\")\n",
    "    print(f\"  Both detected: {results['both_detected']}\")\n",
    "    print(f\"  Only CPU detected: {results['only_cpu_detected']}\")\n",
    "    print(f\"  Only NPU detected: {results['only_npu_detected']}\")\n",
    "    print(f\"  Neither detected: {results['neither_detected']}\")\n",
    "    if results['landmark_distances']:\n",
    "        print(f\"  Avg landmark distance: {results['avg_landmark_distance']:.2f} pixels\")\n",
    "        print(f\"  Max landmark distance: {results['max_landmark_distance']:.2f} pixels\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare landmarks\n",
    "landmark_comparison = None\n",
    "if yunet_npu is not None:\n",
    "    landmark_comparison = compare_landmarks(pairs, yunet_cpu, yunet_npu, max_pairs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Landmark 차이 시각화\n",
    "if landmark_comparison and 'per_point_stats' in landmark_comparison:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Per-point average distance\n",
    "    names = [stat['name'] for stat in landmark_comparison['per_point_stats']]\n",
    "    avgs = [stat['avg'] for stat in landmark_comparison['per_point_stats']]\n",
    "    stds = [stat['std'] for stat in landmark_comparison['per_point_stats']]\n",
    "    \n",
    "    ax1.bar(names, avgs, yerr=stds, capsize=5, alpha=0.7, color='steelblue')\n",
    "    ax1.set_ylabel('Avg Distance (pixels)', fontsize=11)\n",
    "    ax1.set_title('Landmark Distance: CPU vs NPU (per point)', fontsize=12)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overall distance distribution\n",
    "    ax2.hist(landmark_comparison['landmark_distances'], bins=30, alpha=0.7, color='coral')\n",
    "    ax2.axvline(landmark_comparison['avg_landmark_distance'], color='red', \n",
    "                linestyle='--', linewidth=2, label=f\"Mean: {landmark_comparison['avg_landmark_distance']:.2f}\")\n",
    "    ax2.set_xlabel('Total Landmark Distance (pixels)', fontsize=11)\n",
    "    ax2.set_ylabel('Frequency', fontsize=11)\n",
    "    ax2.set_title('Distribution of Landmark Distances', fontsize=12)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('yunet_landmark_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Landmark comparison saved to: yunet_landmark_comparison.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. EdgeFace 임베딩 비교 (PyTorch vs NPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_embeddings(pairs: List[Tuple], detector, edgeface_pytorch, edgeface_npu, \n",
    "                       device: str, max_pairs: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    같은 얼굴에 대한 PyTorch vs NPU 임베딩 비교\n",
    "    \n",
    "    Returns:\n",
    "        Dict with embedding comparison statistics\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'cosine_similarities': [],  # Cosine similarity between PyTorch and NPU embeddings\n",
    "        'l2_distances': [],  # L2 distance\n",
    "        'pytorch_times': [],\n",
    "        'npu_times': [],\n",
    "        'valid_pairs': 0\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n=== Comparing EdgeFace Embeddings (PyTorch vs NPU) ===\")\n",
    "    \n",
    "    sampled_pairs = pairs[:max_pairs] if max_pairs else pairs\n",
    "    \n",
    "    for is_same, img1_path, img2_path in tqdm(sampled_pairs, desc=\"Embedding Comparison\"):\n",
    "        for img_path in [img1_path, img2_path]:\n",
    "            if not os.path.exists(img_path):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Detect and align face\n",
    "                pil_img = Image.open(img_path).convert('RGB')\n",
    "                aligned_face = detector.align(pil_img)\n",
    "                \n",
    "                if aligned_face is None:\n",
    "                    continue\n",
    "                \n",
    "                # Convert to numpy BGR\n",
    "                face_np = np.array(aligned_face)\n",
    "                face_np = cv2.cvtColor(face_np, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                # Extract PyTorch embedding\n",
    "                start_time = time.time()\n",
    "                emb_pytorch = extract_embedding_pytorch(face_np, edgeface_pytorch, device)\n",
    "                pytorch_time = time.time() - start_time\n",
    "                results['pytorch_times'].append(pytorch_time)\n",
    "                \n",
    "                # Extract NPU embedding\n",
    "                start_time = time.time()\n",
    "                emb_npu = extract_embedding_npu(face_np, edgeface_npu)\n",
    "                npu_time = time.time() - start_time\n",
    "                results['npu_times'].append(npu_time)\n",
    "                \n",
    "                # Compare embeddings\n",
    "                cosine_sim = np.dot(emb_pytorch, emb_npu)\n",
    "                l2_dist = np.linalg.norm(emb_pytorch - emb_npu)\n",
    "                \n",
    "                results['cosine_similarities'].append(cosine_sim)\n",
    "                results['l2_distances'].append(l2_dist)\n",
    "                results['valid_pairs'] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    # Calculate statistics\n",
    "    if results['cosine_similarities']:\n",
    "        results['avg_cosine_similarity'] = np.mean(results['cosine_similarities'])\n",
    "        results['std_cosine_similarity'] = np.std(results['cosine_similarities'])\n",
    "        results['min_cosine_similarity'] = np.min(results['cosine_similarities'])\n",
    "        results['avg_l2_distance'] = np.mean(results['l2_distances'])\n",
    "        results['avg_pytorch_time'] = np.mean(results['pytorch_times'])\n",
    "        results['avg_npu_time'] = np.mean(results['npu_times'])\n",
    "    \n",
    "    print(f\"\\nEmbedding Comparison Results:\")\n",
    "    print(f\"  Valid pairs: {results['valid_pairs']}\")\n",
    "    if results['cosine_similarities']:\n",
    "        print(f\"  Avg cosine similarity: {results['avg_cosine_similarity']:.4f}\")\n",
    "        print(f\"  Min cosine similarity: {results['min_cosine_similarity']:.4f}\")\n",
    "        print(f\"  Avg L2 distance: {results['avg_l2_distance']:.4f}\")\n",
    "        print(f\"  Avg PyTorch time: {results['avg_pytorch_time']:.4f}s\")\n",
    "        print(f\"  Avg NPU time: {results['avg_npu_time']:.4f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare embeddings (using CPU detector for consistency)\n",
    "embedding_comparison = None\n",
    "if edgeface_npu is not None:\n",
    "    embedding_comparison = compare_embeddings(\n",
    "        pairs, yunet_cpu, edgeface_pytorch, edgeface_npu, device, max_pairs=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding 비교 시각화\n",
    "if embedding_comparison and 'cosine_similarities' in embedding_comparison:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Cosine similarity distribution\n",
    "    ax1.hist(embedding_comparison['cosine_similarities'], bins=30, alpha=0.7, color='green')\n",
    "    ax1.axvline(embedding_comparison['avg_cosine_similarity'], color='red', \n",
    "                linestyle='--', linewidth=2, \n",
    "                label=f\"Mean: {embedding_comparison['avg_cosine_similarity']:.4f}\")\n",
    "    ax1.set_xlabel('Cosine Similarity', fontsize=11)\n",
    "    ax1.set_ylabel('Frequency', fontsize=11)\n",
    "    ax1.set_title('PyTorch vs NPU Embedding Similarity', fontsize=12)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # L2 distance distribution\n",
    "    ax2.hist(embedding_comparison['l2_distances'], bins=30, alpha=0.7, color='orange')\n",
    "    ax2.axvline(embedding_comparison['avg_l2_distance'], color='red', \n",
    "                linestyle='--', linewidth=2, \n",
    "                label=f\"Mean: {embedding_comparison['avg_l2_distance']:.4f}\")\n",
    "    ax2.set_xlabel('L2 Distance', fontsize=11)\n",
    "    ax2.set_ylabel('Frequency', fontsize=11)\n",
    "    ax2.set_title('PyTorch vs NPU Embedding Distance', fontsize=12)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('edgeface_pytorch_vs_npu_embeddings.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Embedding comparison saved to: edgeface_pytorch_vs_npu_embeddings.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. End-to-End 파이프라인 평가\n",
    "\n",
    "전체 파이프라인 조합별 face verification 성능 평가:\n",
    "1. YuNet CPU + EdgeFace PyTorch\n",
    "2. YuNet NPU + EdgeFace NPU\n",
    "3. YuNet CPU + EdgeFace NPU (cross-compatibility)\n",
    "4. YuNet NPU + EdgeFace PyTorch (cross-compatibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline(pairs: List[Tuple], detector, recognizer, pipeline_name: str,\n",
    "                     device: str = 'cuda', max_pairs: Optional[int] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    End-to-end 파이프라인 평가\n",
    "    \n",
    "    Returns:\n",
    "        Dict with evaluation results (ROC AUC, accuracy, etc.)\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    labels = []\n",
    "    processing_times = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    # Sample pairs\n",
    "    if max_pairs:\n",
    "        positive_pairs = [p for p in pairs if p[0] == True]\n",
    "        negative_pairs = [p for p in pairs if p[0] == False]\n",
    "        \n",
    "        half_pairs = max_pairs // 2\n",
    "        selected_positive = positive_pairs[:half_pairs]\n",
    "        selected_negative = negative_pairs[:half_pairs]\n",
    "        \n",
    "        pairs_to_process = selected_positive + selected_negative\n",
    "    else:\n",
    "        pairs_to_process = pairs\n",
    "    \n",
    "    print(f\"\\n=== Evaluating {pipeline_name} ===\")\n",
    "    print(f\"Processing {len(pairs_to_process)} pairs...\")\n",
    "    \n",
    "    # Determine if using NPU or PyTorch recognizer\n",
    "    is_npu_recognizer = hasattr(recognizer, 'inference_engine')\n",
    "    \n",
    "    for is_same, img1_path, img2_path in tqdm(pairs_to_process, desc=pipeline_name):\n",
    "        if not (os.path.exists(img1_path) and os.path.exists(img2_path)):\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Load images\n",
    "            img1 = Image.open(img1_path).convert('RGB')\n",
    "            img2 = Image.open(img2_path).convert('RGB')\n",
    "            \n",
    "            # Detect and align\n",
    "            aligned1 = detector.align(img1)\n",
    "            aligned2 = detector.align(img2)\n",
    "            \n",
    "            if aligned1 is None or aligned2 is None:\n",
    "                failed_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Convert to numpy BGR\n",
    "            face1_np = cv2.cvtColor(np.array(aligned1), cv2.COLOR_RGB2BGR)\n",
    "            face2_np = cv2.cvtColor(np.array(aligned2), cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Extract embeddings\n",
    "            if is_npu_recognizer:\n",
    "                emb1 = extract_embedding_npu(face1_np, recognizer)\n",
    "                emb2 = extract_embedding_npu(face2_np, recognizer)\n",
    "            else:\n",
    "                emb1 = extract_embedding_pytorch(face1_np, recognizer, device)\n",
    "                emb2 = extract_embedding_pytorch(face2_np, recognizer, device)\n",
    "            \n",
    "            # Calculate similarity\n",
    "            similarity = np.dot(emb1, emb2)\n",
    "            similarities.append(similarity)\n",
    "            labels.append(1 if is_same else 0)\n",
    "            \n",
    "            processing_times.append(time.time() - start_time)\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_count += 1\n",
    "            continue\n",
    "    \n",
    "    # Calculate metrics\n",
    "    similarities = np.array(similarities)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    if len(similarities) == 0:\n",
    "        print(\"No valid pairs processed!\")\n",
    "        return None\n",
    "    \n",
    "    # ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(labels, similarities)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Best accuracy\n",
    "    accuracies = []\n",
    "    for threshold in thresholds:\n",
    "        predictions = (similarities >= threshold).astype(int)\n",
    "        accuracy = np.mean(predictions == labels)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    best_idx = np.argmax(accuracies)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    best_accuracy = accuracies[best_idx]\n",
    "    \n",
    "    # EER\n",
    "    eer_idx = np.nanargmin(np.absolute(fpr - (1 - tpr)))\n",
    "    eer = fpr[eer_idx]\n",
    "    \n",
    "    results = {\n",
    "        'pipeline': pipeline_name,\n",
    "        'num_pairs': len(similarities),\n",
    "        'failed_pairs': failed_count,\n",
    "        'success_rate': len(similarities) / len(pairs_to_process),\n",
    "        'roc_auc': roc_auc,\n",
    "        'best_accuracy': best_accuracy,\n",
    "        'best_threshold': best_threshold,\n",
    "        'eer': eer,\n",
    "        'avg_processing_time': np.mean(processing_times),\n",
    "        'similarities': similarities,\n",
    "        'labels': labels,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'thresholds': thresholds\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{pipeline_name} Results:\")\n",
    "    print(f\"  Pairs processed: {results['num_pairs']}\")\n",
    "    print(f\"  Success rate: {results['success_rate']:.4f}\")\n",
    "    print(f\"  ROC AUC: {results['roc_auc']:.4f}\")\n",
    "    print(f\"  Best Accuracy: {results['best_accuracy']:.4f}\")\n",
    "    print(f\"  Best Threshold: {results['best_threshold']:.4f}\")\n",
    "    print(f\"  EER: {results['eer']:.4f}\")\n",
    "    print(f\"  Avg Processing Time: {results['avg_processing_time']:.4f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate all pipeline combinations\n",
    "pipeline_results = {}\n",
    "\n",
    "# 1. YuNet CPU + EdgeFace PyTorch\n",
    "pipeline_results['CPU_PyTorch'] = evaluate_pipeline(\n",
    "    pairs, yunet_cpu, edgeface_pytorch, \"YuNet CPU + EdgeFace PyTorch\", \n",
    "    device=device, max_pairs=1000\n",
    ")\n",
    "\n",
    "# 2. YuNet NPU + EdgeFace NPU\n",
    "if yunet_npu is not None and edgeface_npu is not None:\n",
    "    pipeline_results['NPU_NPU'] = evaluate_pipeline(\n",
    "        pairs, yunet_npu, edgeface_npu, \"YuNet NPU + EdgeFace NPU\",\n",
    "        max_pairs=1000\n",
    "    )\n",
    "\n",
    "# 3. YuNet CPU + EdgeFace NPU (cross-compatibility)\n",
    "if edgeface_npu is not None:\n",
    "    pipeline_results['CPU_NPU'] = evaluate_pipeline(\n",
    "        pairs, yunet_cpu, edgeface_npu, \"YuNet CPU + EdgeFace NPU\",\n",
    "        max_pairs=1000\n",
    "    )\n",
    "\n",
    "# 4. YuNet NPU + EdgeFace PyTorch (cross-compatibility)\n",
    "if yunet_npu is not None:\n",
    "    pipeline_results['NPU_PyTorch'] = evaluate_pipeline(\n",
    "        pairs, yunet_npu, edgeface_pytorch, \"YuNet NPU + EdgeFace PyTorch\",\n",
    "        device=device, max_pairs=1000\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 종합 결과 비교 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 성능 비교 테이블\n",
    "if pipeline_results:\n",
    "    comparison_data = []\n",
    "    for name, result in pipeline_results.items():\n",
    "        if result is not None:\n",
    "            comparison_data.append({\n",
    "                'Pipeline': result['pipeline'],\n",
    "                'Success Rate': f\"{result['success_rate']:.4f}\",\n",
    "                'ROC AUC': f\"{result['roc_auc']:.4f}\",\n",
    "                'Best Accuracy': f\"{result['best_accuracy']:.4f}\",\n",
    "                'Best Threshold': f\"{result['best_threshold']:.4f}\",\n",
    "                'EER': f\"{result['eer']:.4f}\",\n",
    "                'Avg Time (s)': f\"{result['avg_processing_time']:.4f}\"\n",
    "            })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\n=== Pipeline Performance Comparison ===\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # CSV로 저장\n",
    "    comparison_df.to_csv('pipeline_comparison.csv', index=False)\n",
    "    print(\"\\nComparison saved to: pipeline_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC 커브 비교\n",
    "if pipeline_results:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange']\n",
    "    linestyles = ['-', '-', '--', '--']\n",
    "    \n",
    "    for (name, result), color, linestyle in zip(pipeline_results.items(), colors, linestyles):\n",
    "        if result is not None:\n",
    "            plt.plot(result['fpr'], result['tpr'], \n",
    "                    label=f\"{result['pipeline']} (AUC = {result['roc_auc']:.4f})\",\n",
    "                    linewidth=2, color=color, linestyle=linestyle)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')\n",
    "    \n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title('ROC Curves: Pipeline Comparison', fontsize=14)\n",
    "    plt.legend(fontsize=10, loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig('pipeline_roc_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"ROC comparison saved to: pipeline_roc_comparison.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 메트릭 비교 막대 그래프\n",
    "if pipeline_results:\n",
    "    metrics_to_plot = ['roc_auc', 'best_accuracy', 'eer']\n",
    "    metric_names = ['ROC AUC', 'Best Accuracy', 'EER']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    \n",
    "    for ax, metric, metric_name in zip(axes, metrics_to_plot, metric_names):\n",
    "        pipelines = []\n",
    "        values = []\n",
    "        \n",
    "        for name, result in pipeline_results.items():\n",
    "            if result is not None:\n",
    "                pipelines.append(name)\n",
    "                values.append(result[metric])\n",
    "        \n",
    "        bars = ax.bar(pipelines, values, alpha=0.7)\n",
    "        ax.set_ylabel(metric_name, fontsize=11)\n",
    "        ax.set_title(f'{metric_name} Comparison', fontsize=12)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 값 레이블\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.4f}',\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pipeline_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Metrics comparison saved to: pipeline_metrics_comparison.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 결론 및 분석\n",
    "\n",
    "이 노트북을 통해 다음을 분석할 수 있습니다:\n",
    "\n",
    "### 1. YuNet 검출 성능\n",
    "- CPU vs NPU 검출률 차이\n",
    "- Landmark 정확도 차이\n",
    "- Detection threshold의 영향\n",
    "\n",
    "### 2. EdgeFace 임베딩 품질\n",
    "- PyTorch vs NPU 임베딩 유사도\n",
    "- Quantization으로 인한 정확도 손실\n",
    "\n",
    "### 3. Cross-compatibility\n",
    "- CPU 캡처 얼굴을 NPU로 인식 가능 여부\n",
    "- NPU 캡처 얼굴을 CPU로 인식 가능 여부\n",
    "- 호환성 문제의 원인 파악\n",
    "\n",
    "### 4. 최적 파이프라인 선택\n",
    "- 정확도 vs 속도 trade-off\n",
    "- Production 환경에 적합한 조합 결정"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
